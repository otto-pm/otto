<<<<<<< HEAD
# OTTO

AI-powered project management tool with repository-aware Q&A and automated task generation.

## Features

- **Q&A Agent**: Ask questions about your codebase and get contextual answers via RAG
- **Task Generation**: Input product requirements, receive structured Kanban tasks as JSON
- **GitHub Integration**: Connect repositories, auto-sync on push to main

## Tech Stack

| Layer | Technologies |
|-------|--------------|
| Frontend | Next.js 14, React, Tailwind CSS |
| Backend | FastAPI, Python 3.11 |
| Database | Firestore, Cloud Storage |
| ML/AI | Vertex AI (Gemini 1.5 Pro, fine-tuned with LoRA), Vector Search (ScaNN), Embeddings API |
| Infrastructure | GCP, Cloud Run, Terraform, Cloud Build |

## Prerequisites

- Node.js 18+
- Python 3.11+
- Docker
- GCP account with billing enabled
- GitHub account

## Setup

### 1. Clone and Install

```bash
git clone https://github.com/otto-pm/otto
cd otto

# Frontend
cd frontend && npm install

# Backend
cd ../backend && pip install -r requirements.txt
```

### 2. Configure Environment

```bash
# Copy environment templates
cp frontend/.env.example frontend/.env.local
cp backend/.env.example backend/.env

# Required variables:
# - GOOGLE_CLOUD_PROJECT
# - FIREBASE_API_KEY
# - GITHUB_APP_ID
# - GITHUB_PRIVATE_KEY
```

### 3. Deploy Infrastructure

```bash
cd infrastructure/terraform
terraform init
terraform apply -var-file=environments/staging.tfvars
```

### 4. Run Locally

```bash
# Terminal 1 - Backend
cd backend && uvicorn src.main:app --reload

# Terminal 2 - Frontend
cd frontend && npm run dev
```

App available at `http://localhost:3000`

## Usage

### Connect a Repository
1. Sign in with GitHub
2. Click "Connect Repository"
3. Select repository and authorize access
4. Wait for initial indexing to complete

### Q&A Mode
```
[QANDA] How does the authentication middleware work?
```

### Task Generation Mode
```
[TASKGEN] Build user login with OAuth support
```

Returns structured JSON with tasks, acceptance criteria, and estimates.

## Project Structure

```
otto/
â”œâ”€â”€ frontend/          # Next.js app
â”œâ”€â”€ backend/           # FastAPI services
â”œâ”€â”€ ml/                # Pipelines, fine-tuning, evaluation
â”œâ”€â”€ infrastructure/    # Terraform, Cloud Build configs
â”œâ”€â”€ tests/             # Unit and integration tests
â””â”€â”€ docs/              # Documentation
```

## Development

```bash
# Run tests
cd backend && pytest
cd frontend && npm test

# Build Docker images
docker build -t otto-backend ./backend
docker build -t otto-ml ./ml
```

## Cloud Setup

1. Clone the repo: `git clone https://github.com/otto-pm/otto.git`
2. Authenticate with GCP: `gcloud auth login`
3. Set the project: `gcloud config set project otto-pm`
4. Run the setup script:
   - **Windows:** `setup-env.bat`
   - **Mac/Linux:** `chmod +x setup-env.sh && ./setup-env.sh`
=======
>>>>>>> 155f254 (Squashed 'ingest-service/' content from commit 2198208)
Otto - AI-Powered Code RAG System
=================================

**Comprehensive data pipeline for GitHub repository ingestion, intelligent chunking, and RAG-based code assistance.**

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/) [![GCP](https://img.shields.io/badge/cloud-GCP-4285F4.svg)](https://cloud.google.com/) [![License](https://img.shields.io/badge/license-MIT-green.svg)](https://claude.ai/chat/LICENSE)

* * * * *

ğŸ“‹ Table of Contents
--------------------

-   [Overview](https://claude.ai/chat/6cca505e-43f0-4430-9c40-6c2e977214f0#overview)
-   [Features](https://claude.ai/chat/6cca505e-43f0-4430-9c40-6c2e977214f0#features)
-   [Architecture](https://claude.ai/chat/6cca505e-43f0-4430-9c40-6c2e977214f0#architecture)
-   [Prerequisites](https://claude.ai/chat/6cca505e-43f0-4430-9c40-6c2e977214f0#prerequisites)
-   [Installation](https://claude.ai/chat/6cca505e-43f0-4430-9c40-6c2e977214f0#installation)
-   [Configuration](https://claude.ai/chat/6cca505e-43f0-4430-9c40-6c2e977214f0#configuration)
-   [Usage](https://claude.ai/chat/6cca505e-43f0-4430-9c40-6c2e977214f0#usage)
    -   [1\. Repository Ingestion](https://claude.ai/chat/6cca505e-43f0-4430-9c40-6c2e977214f0#1-repository-ingestion)
    -   [2\. Code Chunking](https://claude.ai/chat/6cca505e-43f0-4430-9c40-6c2e977214f0#2-code-chunking)
    -   [3\. Embedding Generation](https://claude.ai/chat/6cca505e-43f0-4430-9c40-6c2e977214f0#3-embedding-generation)
    -   [4\. RAG Services](https://claude.ai/chat/6cca505e-43f0-4430-9c40-6c2e977214f0#4-rag-services)
-   [Project Structure](https://claude.ai/chat/6cca505e-43f0-4430-9c40-6c2e977214f0#project-structure)
-   [API Reference](https://claude.ai/chat/6cca505e-43f0-4430-9c40-6c2e977214f0#api-reference)
-   [Troubleshooting](https://claude.ai/chat/6cca505e-43f0-4430-9c40-6c2e977214f0#troubleshooting)
-   [Contributing](https://claude.ai/chat/6cca505e-43f0-4430-9c40-6c2e977214f0#contributing)
-   [License](https://claude.ai/chat/6cca505e-43f0-4430-9c40-6c2e977214f0#license)

* * * * *

ğŸ¯ Overview
-----------

Otto is an intelligent code analysis system that:

1.  **Ingests** GitHub repositories into Google Cloud Storage
2.  **Chunks** code semantically with rich context extraction
3.  **Embeds** chunks using Vertex AI for semantic search
4.  **Provides** RAG-based services: Q&A, Documentation, Code Completion, and Code Editing

Built for the **Otto Project** - a software engineering project management solution leveraging LLMs and RAG.

* * * * *

âœ¨ Features
----------

### ğŸ”„ Repository Ingestion

-   âœ… GitHub API integration with OAuth support
-   âœ… Automatic file filtering (code files only)
-   âœ… Metadata extraction and storage
-   âœ… Support for multiple programming languages

### ğŸ§© Intelligent Chunking

-   âœ… **Semantic chunking** using tree-sitter for Python, JavaScript, TypeScript, Java, Go
-   âœ… **Context enrichment**: type hints, docstrings, decorators, imports, exceptions
-   âœ… **Large chunks** (150 lines) for better LLM understanding
-   âœ… **Overlap** between chunks for continuity

### ğŸ¯ Vector Embeddings

-   âœ… Vertex AI text-embedding-004 model
-   âœ… Batch processing (25 chunks at once)
-   âœ… Efficient retry and error handling
-   âœ… 100% embedding coverage

### ğŸ¤– RAG Services

-   âœ… **Q&A**: Answer questions about codebase
-   âœ… **Documentation**: Generate API docs, user guides, technical docs, READMEs
-   âœ… **Code Completion**: Intelligent suggestions based on patterns
-   âœ… **Code Editing**: Modify code with instructions
-   âœ… **Streaming support** for real-time responses

* * * * *

ğŸ—ï¸ Architecture
----------------

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   GitHub Repository                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              1. INGESTION (GitHub â†’ GCS)                 â”‚
â”‚  - Fetch repo via GitHub API                             â”‚
â”‚  - Filter code files                                     â”‚
â”‚  - Store in otto-raw-repos bucket                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           2. CHUNKING (Enhanced Context)                 â”‚
â”‚  - Semantic chunking (tree-sitter)                       â”‚
â”‚  - Extract: types, docstrings, decorators, imports      â”‚
â”‚  - Build enriched context for LLMs                       â”‚
â”‚  - Store in otto-processed-chunks bucket                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        3. EMBEDDING (Vertex AI text-embedding-004)       â”‚
â”‚  - Batch generation (25 at a time)                       â”‚
â”‚  - 768-dimensional vectors                               â”‚
â”‚  - Update chunks with embeddings                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              4. RAG SERVICES (Gemini)                    â”‚
â”‚  - Vector search (semantic similarity)                   â”‚
â”‚  - Context retrieval (top-k chunks)                      â”‚
â”‚  - LLM generation (Gemini 1.5 Flash)                     â”‚
â”‚  - Q&A | Docs | Completion | Editing                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

* * * * *

ğŸ“‹ Prerequisites
----------------

### Required

-   **Python**: 3.11 or higher
-   **GCP Account**: With billing enabled
-   **GitHub Account**: For repository access
-   **Gemini API Key**: Free from [Google AI Studio](https://aistudio.google.com/app/apikey)

### GCP Services Required

-   Cloud Storage
-   Vertex AI (for embeddings)
-   IAM & Admin

* * * * *

ğŸš€ Installation
---------------

### 1\. Clone the Repository

```
git clone https://github.com/Malav2002/ingest_repo.git
cd ingest_repo

```

### 2\. Set Up Python Environment

```
# Deactivate conda if active
conda deactivate

# Create virtual environment
python3.11 -m venv venv

# Activate virtual environment
source venv/bin/activate  # Mac/Linux
# or
venv\Scripts\activate  # Windows

```

### 3\. Install Dependencies

```
# Upgrade pip
pip install --upgrade pip

# Install all requirements
pip install -r requirements.txt

# Install Gemini API SDK
pip install google-generativeai

```

### 4\. Install Google Cloud SDK

```
# Mac
brew install google-cloud-sdk

# Or download from:
# https://cloud.google.com/sdk/docs/install

```

### 5\. Authenticate with Google Cloud

```
# Login to GCP
gcloud auth login

# Set up application default credentials
gcloud auth application-default login

# Set your project
gcloud config set project YOUR_PROJECT_ID

```

* * * * *

âš™ï¸ Configuration
----------------

### 1\. GCP Setup

```
# Set your project ID
export PROJECT_ID="your-gcp-project-id"

# Enable required APIs
gcloud services enable\
  cloudfunctions.googleapis.com\
  storage.googleapis.com\
  aiplatform.googleapis.com\
  compute.googleapis.com

# Create storage buckets
gsutil mb -p $PROJECT_ID -l us-central1 gs://otto-raw-repos
gsutil mb -p $PROJECT_ID -l us-central1 gs://otto-processed-chunks
gsutil mb -p $PROJECT_ID -l us-central1 gs://otto-dataflow-temp

```

### 2\. Get API Keys

#### GitHub Token

1.  Go to: https://github.com/settings/tokens
2.  Generate new token (classic)
3.  Select scope: `repo` (full control)
4.  Copy the token

#### Gemini API Key

1.  Go to: https://aistudio.google.com/app/apikey
2.  Click "Create API Key"
3.  Copy the key

### 3\. Configure Environment Variables

Create a `.env` file in the project root:

```
# .env file
PROJECT_ID=your-gcp-project-id
LOCATION=us-central1
BUCKET_RAW=otto-raw-repos
BUCKET_PROCESSED=otto-processed-chunks
BUCKET_TEMP=otto-dataflow-temp
GITHUB_TOKEN=your_github_token_here
GEMINI_API_KEY=your_gemini_api_key_here

```

### 4\. Create `.gitignore`

```
# .gitignore
credentials.json
*.json
.env
venv/
__pycache__/
*.pyc
.DS_Store
*.log

```

* * * * *

ğŸ“š Usage
--------

### 1\. Repository Ingestion

Ingest a GitHub repository into Cloud Storage:

```
# Ingest a public repository
python scripts/ingest_repo.py owner/repository-name

# Example
python scripts/ingest_repo.py malav2002/ai-portfolio-analyzer

# Ingest a specific branch
python scripts/ingest_repo.py owner/repo --branch develop

# View help
python scripts/ingest_repo.py --help

```

**Output:**

-   Raw files stored in: `gs://otto-raw-repos/owner/repo/`
-   Metadata: `gs://otto-raw-repos/owner/repo/metadata.json`

* * * * *

### 2\. Code Chunking

Process ingested repository into intelligent chunks:

```
# Basic chunking (enhanced with context)
python scripts/process_repo.py owner/repository-name

# Example
python scripts/process_repo.py malav2002/ai-portfolio-analyzer

# Custom chunk size
python scripts/process_repo.py owner/repo --chunk-size 200 --overlap 15

# Use basic chunker (faster, less context)
python scripts/process_repo.py owner/repo --basic

# View help
python scripts/process_repo.py --help

```

**What happens:**

-   âœ… Semantic chunking with tree-sitter
-   âœ… Context extraction (types, docstrings, imports, decorators)
-   âœ… Enriched content for LLM understanding
-   âœ… Chunks saved to: `gs://otto-processed-chunks/owner/repo/chunks.jsonl`

**Typical output:**

```
ğŸ”§ Processing repository: malav2002/ai-portfolio-analyzer
ğŸ“ Processing 35 files with larger chunk size (150 lines)
âœ“ 35/35 files (285 chunks, 7.0 files/sec)
âš¡ Chunking completed in 5.0s
âœ… Created 285 context-rich chunks

```

* * * * *

### 3\. Embedding Generation

Generate embeddings for semantic search:

```
# Generate embeddings for all chunks
python scripts/embed_repo.py owner/repository-name

# Example
python scripts/embed_repo.py malav2002/ai-portfolio-analyzer

# Force re-embed existing embeddings
python scripts/embed_repo.py owner/repo --force

# Custom batch size
python scripts/embed_repo.py owner/repo --batch-size 50

# View help
python scripts/embed_repo.py --help

```

**What happens:**

-   âœ… Loads chunks from GCS
-   âœ… Generates embeddings using Vertex AI (text-embedding-004)
-   âœ… Batch processing for efficiency
-   âœ… Updates chunks with 768-dimensional vectors

**Typical output:**

```
ğŸ“¦ Loaded: 285 chunks
ğŸ¯ Chunks to embed: 285
ğŸ”„ Generating embeddings (batch size: 25)...
  âœ“ 285/285 embeddings (12.5/sec)
âœ… Generated 285 embeddings in 22.8s

```

* * * * *

### 4\. RAG Services

Use the RAG system for code assistance:

#### 4.1. Q&A Service

Answer questions about your codebase:

```
# Ask a question
python scripts/rag_cli.py owner/repo\
  --service qa\
  --question "How does the OCR service handle errors?"

# With streaming (see response in real-time)
python scripts/rag_cli.py owner/repo\
  --service qa\
  --question "What caching mechanism is used?"\
  --stream

# Filter by language
python scripts/rag_cli.py owner/repo\
  --service qa\
  --question "How is authentication implemented?"\
  --language python

```

**Example:**

```
python scripts/rag_cli.py malav2002/ai-portfolio-analyzer\
  --service qa\
  --question "How does the OCR service handle errors?"\
  --stream

```

#### 4.2. Documentation Generation

Generate professional documentation:

```
# Generate API documentation
python scripts/rag_cli.py owner/repo\
  --service doc\
  --target "portfolio analysis API"\
  --doc-type api\
  --stream

# Generate user guide
python scripts/rag_cli.py owner/repo\
  --service doc\
  --target "getting started"\
  --doc-type user_guide\
  --stream

# Generate technical documentation
python scripts/rag_cli.py owner/repo\
  --service doc\
  --target "OCR service architecture"\
  --doc-type technical\
  --stream

# Generate README
python scripts/rag_cli.py owner/repo\
  --service doc\
  --target "project overview"\
  --doc-type readme\
  --stream

```

**Documentation types:**

-   `api` - API reference with function signatures
-   `user_guide` - Step-by-step user instructions
-   `technical` - Technical architecture and implementation
-   `readme` - Complete README.md

#### 4.3. Code Completion

Get intelligent code suggestions:

```
# Complete code
python scripts/rag_cli.py owner/repo\
  --service complete\
  --code "async def process_portfolio(image_path: str):"\
  --language python\
  --stream

# Example
python scripts/rag_cli.py malav2002/ai-portfolio-analyzer\
  --service complete\
  --code "def extract_text(image):"\
  --language python

```

#### 4.4. Code Editing

Modify existing code based on instructions:

```
# Edit code with instructions
python scripts/rag_cli.py owner/repo\
  --service edit\
  --file "services/ocr_service.py"\
  --instruction "add retry logic and better error handling"\
  --stream

# Example
python scripts/rag_cli.py malav2002/ai-portfolio-analyzer\
  --service edit\
  --file "ml-service/src/services/ocr_service.py"\
  --instruction "add rate limiting"\
  --stream

```

* * * * *

ğŸ“ Project Structure
--------------------

```
ingest_repo/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ github_ingester.py        # GitHub API integration
â”‚   â”œâ”€â”€ chunking/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ chunker.py                # Basic chunker
â”‚   â”‚   â”œâ”€â”€ enhanced_chunker.py       # Enhanced context extraction
â”‚   â”‚   â””â”€â”€ embedder.py               # Embedding generation
â”‚   â””â”€â”€ rag/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ llm_client_gemini_api.py  # Gemini API client
â”‚       â”œâ”€â”€ vector_search.py          # Semantic search
â”‚       â””â”€â”€ rag_services.py           # Q&A, Docs, Completion, Editing
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ ingest_repo.py                # CLI: Ingest repository
â”‚   â”œâ”€â”€ process_repo.py               # CLI: Chunk repository
â”‚   â”œâ”€â”€ embed_repo.py                 # CLI: Generate embeddings
â”‚   â”œâ”€â”€ rag_cli.py                    # CLI: RAG services
â”‚   â”œâ”€â”€ inspect_chunks.py             # Analyze chunk quality
â”‚   â””â”€â”€ analyze_chunk_quality.py      # Quality metrics
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_ingestion.py
â”‚   â””â”€â”€ test_chunking.py
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ .env                              # Environment variables (don't commit!)
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

```

* * * * *

ğŸ”§ API Reference
----------------

### Ingestion

```
from src.ingestion.github_ingester import GitHubIngester

ingester = GitHubIngester(
    project_id="your-project-id",
    bucket_name="otto-raw-repos",
    github_token="your_token"
)

metadata = ingester.ingest_repository("owner/repo", branch="main")

```

### Chunking

```
from src.chunking.enhanced_chunker import EnhancedCodeChunker

chunker = EnhancedCodeChunker(
    project_id="your-project-id",
    bucket_raw="otto-raw-repos",
    bucket_processed="otto-processed-chunks"
)

chunks = chunker.process_repository("owner/repo")

```

### Embeddings

```
from src.chunking.embedder import ChunkEmbedder

embedder = ChunkEmbedder(
    project_id="your-project-id",
    bucket_processed="otto-processed-chunks"
)

stats = embedder.embed_repository("owner/repo")

```

### RAG Services

```
from src.rag.rag_services import RAGServices

rag = RAGServices(
    project_id="your-project-id",
    bucket_processed="otto-processed-chunks"
)

# Q&A
result = rag.answer_question("How does X work?", "owner/repo")

# Documentation
docs = rag.generate_documentation("API", "owner/repo", doc_type="api")

# Code completion
completion = rag.complete_code("def process_", "", "owner/repo", "python")

# Code editing
edited = rag.edit_code("add error handling", "file.py", "owner/repo")

```

* * * * *

ğŸ› Troubleshooting
------------------

### Common Issues

#### 1\. **Authentication Errors**

```
# Re-authenticate
gcloud auth application-default login

# Check credentials
gcloud auth list

```

#### 2\. **Bucket Not Found**

```
# List buckets
gsutil ls

# Create missing buckets
gsutil mb gs://otto-raw-repos
gsutil mb gs://otto-processed-chunks

```

#### 3\. **API Not Enabled**

```
# Enable required APIs
gcloud services enable aiplatform.googleapis.com
gcloud services enable storage.googleapis.com

```

#### 4\. **Gemini API Key Issues**

```
# Verify key is set
echo $GEMINI_API_KEY

# Get new key from: https://aistudio.google.com/app/apikey

```

#### 5\. **Embedding Timeouts**

```
# Check network connectivity
ping us-central1-aiplatform.googleapis.com

# Try different region
python scripts/embed_repo.py owner/repo --location us-east4

```

#### 6\. **Module Not Found**

```
# Reinstall dependencies
pip install -r requirements.txt
pip install google-generativeai

```

* * * * *

ğŸ“Š Performance Metrics
----------------------

Based on testing with **malav2002/ai-portfolio-analyzer** (35 files):

| Metric | Value |
| --- | --- |
| **Ingestion Speed** | ~6 files/sec |
| **Chunking Speed** | ~7 files/sec |
| **Total Chunks** | 285 |
| **Avg Chunk Size** | 2,784 chars |
| **Embedding Speed** | ~12/sec |
| **Semantic Coverage** | 95.4% |
| **Import Context** | 68.4% |

**Quality Scores:**

-   âœ… **Documentation Generation**: Excellent (high semantic chunks)
-   âœ… **Code Completion**: Good (70% focused chunks)
-   âœ… **Q&A Search**: Excellent (100% embeddings)

* * * * *

ğŸ’° Cost Estimate
----------------

### Free Tier (Recommended)

-   **Gemini API**: FREE (15 req/min, 1M tokens/day)
-   **Cloud Storage**: $0.02/GB/month (~$0.50/month for typical use)
-   **Vertex AI Embeddings**: ~$0.025 per 1K embeddings (~$7 for 285 chunks)

**Total**: ~$8/month for moderate use

### Production Tier

-   **Gemini 1.5 Pro**: $1.25 per 1M input tokens
-   Scalable based on usage

* * * * *

ğŸ¤ Contributing
---------------

Contributions welcome! Please follow these steps:

1.  Fork the repository
2.  Create a feature branch: `git checkout -b feature/amazing-feature`
3.  Commit changes: `git commit -m 'Add amazing feature'`
4.  Push to branch: `git push origin feature/amazing-feature`
5.  Open a Pull Request

* * * * *

ğŸ“ License
----------

This project is part of the **Otto** software engineering project management system.

* * * * *

ğŸ™ Acknowledgments
------------------

-   **Google Cloud Platform** for infrastructure
-   **Vertex AI** for embeddings
-   **Gemini** for LLM capabilities
-   **tree-sitter** for semantic parsing

* * * * *

ğŸ“ Support
----------

For issues and questions:

-   Open an issue on GitHub
-   Contact: me

* * * * *

ğŸš€ Quick Start Summary
----------------------

```
# 1. Setup
git clone https://github.com/Malav2002/ingest_repo.git
cd ingest_repo
python3.11 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
pip install google-generativeai

# 2. Configure
# Add to .env: PROJECT_ID, GEMINI_API_KEY, GITHUB_TOKEN

# 3. Create GCP resources
gcloud services enable aiplatform.googleapis.com storage.googleapis.com
gsutil mb gs://otto-raw-repos
gsutil mb gs://otto-processed-chunks

# 4. Use the pipeline
python scripts/ingest_repo.py owner/repo
python scripts/process_repo.py owner/repo
python scripts/embed_repo.py owner/repo

# 5. Ask questions!
python scripts/rag_cli.py owner/repo\
  --service qa\
  --question "How does this work?"\
  --stream

```

* * * * *

**Built with â¤ï¸ for the Otto Project**
# webhook test
